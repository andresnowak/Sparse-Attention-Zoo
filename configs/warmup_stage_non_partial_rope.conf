# Default training configuration
# Source this file in your sbatch script: source configs/default.config
STAGE_NAME="warmup"

# Model configuration
MODEL_NAME="meta-llama/Llama-3.2-1B"
INDEX_TOP_K=512
INDEX_NUM_HEADS=16
ROPE_HEAD_DIM=64
INDEX_HEAD_DIM=64

# Training configuration
MICRO_BATCH_SIZE=4
GLOBAL_BATCH_SIZE=32
LEARNING_RATE=1e-3
MIN_LR=1e-4  # Minimum learning rate for cosine annealing scheduler
WARMUP_START_LR=1e-5  # Starting learning rate for linear warmup
WARMUP_RATIO=0.1  # Ratio of total steps used for warmup
NUM_EPOCHS=1
MAX_SEQ_LENGTH=2048

WARMUP_STAGE="--warmup_stage"

# Dataset configuration
# DATASET_NAME="allenai/olmo-mix-1124"
# DATASET_CONFIG="arxiv"
# DATASET_SPLIT="train"
DATASET_NAME="HuggingFaceFW/fineweb-edu"
DATASET_CONFIG="sample-100BT"
DATASET_SPLIT="train"
# MAX_TRAIN_SAMPLES=128000 # 8000 steps with a real batch size of 16
# MAX_TRAIN_TOKENS=100000000 # 100_000_000
# MAX_TRAIN_TOKENS=500000000 # 500_000_000
MAX_TRAIN_TOKENS=2000000000 # 2_000_000_000  100,000 samples for 2048 seq length
DATASET_OFFSET=0

# Logging and checkpointing
WANDB_RUN_NAME="llama-dsa-warmup-stage_non-partial-rope-indexer"
SAVE_EVERY=5000
LOG_EVERY=10
TRACK_LOG_EVERY=1000

# Loss configuration
WEIGHT_DECAY=0
GRADIENT_CLIPPING=1.0
