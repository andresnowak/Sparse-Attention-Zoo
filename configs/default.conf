# Default training configuration
# Source this file in your sbatch script: source configs/default.config

# Model configuration
MODEL_NAME="meta-llama/Llama-3.2-1B"
INDEX_TOP_K=2048
INDEX_NUM_HEADS=16
ROPE_HEAD_DIM=32
INDEX_HEAD_DIM=64

# Training configuration
BATCH_SIZE=4
LEARNING_RATE=1e-4
NUM_EPOCHS=3
MAX_SEQ_LENGTH=2048
GRADIENT_ACCUMULATION_STEPS=4

# Dataset configuration
DATASET_NAME="wikitext"
DATASET_CONFIG="wikitext-2-raw-v1"
DATASET_SPLIT="train"
MAX_TRAIN_SAMPLES=10000

# Logging and checkpointing
WANDB_RUN_NAME="llama-dsa"
SAVE_DIR="$SCRATCH/Sparse-Attention-Zoo/checkpoints/default"
SAVE_EVERY=1000
LOG_EVERY=10

# Loss configuration
WEIGHT_DECAY=0.1
