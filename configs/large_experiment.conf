# Large-scale experiment configuration
# Source this file in your sbatch script: source configs/large_experiment.config

# Model configuration
MODEL_NAME="meta-llama/Llama-3.2-1B"
INDEX_TOP_K=4096
INDEX_NUM_HEADS=32
ROPE_HEAD_DIM=32
INDEX_HEAD_DIM=64

# Training configuration
BATCH_SIZE=2
LEARNING_RATE=5e-5
NUM_EPOCHS=10
MAX_SEQ_LENGTH=4096
GRADIENT_ACCUMULATION_STEPS=8

# Dataset configuration
DATASET_NAME="allenai/c4"
DATASET_CONFIG="en"
DATASET_SPLIT="train"
MAX_TRAIN_SAMPLES=100000

# Logging and checkpointing
WANDB_RUN_NAME="large-c4-experiment"
SAVE_DIR="$SCRATCH/Sparse-Attention-Zoo/checkpoints/large-c4"
SAVE_EVERY=500
LOG_EVERY=50

# Loss configuration
WEIGHT_DECAY=0.1
