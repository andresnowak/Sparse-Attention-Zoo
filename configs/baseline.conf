# Default training configuration
# Source this file in your sbatch script: source configs/default.config
STAGE_NAME="baseline"

# Model configuration
MODEL_NAME="meta-llama/Llama-3.2-1B"
INDEX_TOP_K=128
INDEX_NUM_HEADS=16
ROPE_HEAD_DIM=32
INDEX_HEAD_DIM=64

# Training configuration
MICRO_BATCH_SIZE=4
GLOBAL_BATCH_SIZE=16
LEARNING_RATE=1e-4
NUM_EPOCHS=1
MAX_SEQ_LENGTH=1024

BASELINE_EXPERIMENT="--baseline_experiment"

# Dataset configuration
DATASET_NAME="allenai/olmo-mix-1124"
DATASET_CONFIG="arxiv"
DATASET_SPLIT="train"
MAX_TRAIN_SAMPLES=128000 # 8000 steps with a real batch size of 16
DATASET_OFFSET=0

# Logging and checkpointing
WANDB_RUN_NAME="llama-dsa-baseline"
SAVE_EVERY=1000
LOG_EVERY=10

# Loss configuration
WEIGHT_DECAY=0 # For llama 3.0 and 3.1 at least the weight decay they used was 0.1 times the learning rate at that step
GRADIENT_CLIPPING=1.0
